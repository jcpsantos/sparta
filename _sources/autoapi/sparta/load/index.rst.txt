:py:mod:`sparta.load`
=====================

.. py:module:: sparta.load


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   sparta.load.save_df_azure_dw
   sparta.load.create_hive_table



.. py:function:: save_df_azure_dw(df: pyspark.sql.DataFrame, url_jdbc: str, tempdir: str, table: str, mode: str = 'overwrite', max_str_length: int = 4000) -> None

   Write a PySpark DataFrame to an Azure SQL DW table.

   :param df: The PySpark DataFrame to be written.
   :type df: DataFrame
   :param url_jdbc: The JDBC connection URL for the Azure SQL DW.
   :type url_jdbc: str
   :param tempdir: Path for writing temporary files.
   :type tempdir: str
   :param table: The name of the table where the DataFrame will be written.
   :type table: str
   :param mode: The mode for writing the table (overwrite or append). Defaults to 'overwrite'.
   :type mode: str, optional
   :param max_str_length: The maximum string length for all NVARCHAR columns. Defaults to 4000.
   :type max_str_length: int, optional

   :raises ValueError: If the mode parameter is not 'overwrite' or 'append'.
   :raises Exception: If the DataFrame cannot be written to the Azure SQL DW.


.. py:function:: create_hive_table(df: pyspark.sql.DataFrame, table: str, num_buckets: int, *grouping_columns: str) -> None

   Transform a DataFrame into a table in the Spark Warehouse.

   :param df: The DataFrame to transform.
   :type df: DataFrame
   :param table: The name of the table to create.
   :type table: str
   :param num_buckets: The number of buckets to save the table.
   :type num_buckets: int
   :param \*grouping_columns: The names of the columns to group by.
   :type \*grouping_columns: str

   :returns: None.

   .. rubric:: Example

   >>> create_hive_table(df, "table_name", 5, "col1", "col2", "col3")


