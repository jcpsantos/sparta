:py:mod:`sparta.extract`
========================

.. py:module:: sparta.extract


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   sparta.extract.read_with_schema
   sparta.extract.read_yaml_df



.. py:function:: read_with_schema(path: str, schema: str, options: Dict[Any, Any] = None, format: str = 'csv', spark: pyspark.sql.SparkSession = None) -> pyspark.sql.DataFrame

   Function to read DataFrames with predefined schema.
   :param path: Path where the file is located.
   :type path: str
   :param schema: Pre-defined schema for reading.
   :type schema: str
   :param options: Configuration options for reading the DataFrame.
   :type options: dict
   :param format: Format of the file to be read. Defaults to 'csv'.
   :type format: str, optional
   :param spark: Spark session. Defaults to None.
   :type spark: SparkSession, optional

   :returns: DataFrame read with predefined schema.
   :rtype: DataFrame

   .. rubric:: Example

   >>> schema = 'epidemiological_week LONG, date DATE, order_for_place INT, state STRING, city STRING, city_ibge_code LONG, place_type STRING, last_available_confirmed INT'
   >>> path = '/content/sample_data/covid19-e0534be4ad17411e81305aba2d9194d9.csv'
   >>> df = read_with_schema(path, schema, {'header': 'true'}, 'csv')


.. py:function:: read_yaml_df(path: str, spark: pyspark.sql.SparkSession = None) -> pyspark.sql.DataFrame

   Read a YAML file as a Spark DataFrame.

   :param path: Path of the YAML file.
   :type path: str
   :param spark: Spark session to use. Defaults to spark.
   :type spark: SparkSession, optional

   :returns: YAML file converted to a DataFrame.
   :rtype: DataFrame


