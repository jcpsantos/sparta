sparta.logs
===========

.. py:module:: sparta.logs


Functions
---------

.. autoapisummary::

   sparta.logs.getlogger
   sparta.logs.spark_property_calculator
   sparta.logs.send_error_to_teams
   sparta.logs.handle_exceptions


Module Contents
---------------

.. py:function:: getlogger(name: str, level: logging = logging.INFO) -> logging

   Function that generates custom logs.
   :param name: Run name.
   :type name: str
   :param level: Log level. Defaults to logging.INFO.
   :type level: logging, optional

   :returns: Custom log.
   :rtype: logging

   .. rubric:: Example

   >>> logger = getlogger('test')
   >>> logger.info('test logs')


.. py:function:: spark_property_calculator(number_of_nodes: int, cores_per_node: int, total_memory_per_node: int, spark_executors_cores: int = 5, memory_fraction: float = 0.9) -> Dict[str, Any]

   Calculates the optimal Spark property configuration based on the number of nodes, cores per node,
   and the total available memory per node.

   This function provides the recommended Spark settings for `--executor-cores`, `--executor-memory`,
   and `--num-executors` based on cluster configuration.

   :param number_of_nodes: The total number of nodes in the Spark cluster.
   :type number_of_nodes: int
   :param cores_per_node: The number of CPU cores available on each node.
   :type cores_per_node: int
   :param total_memory_per_node: The total amount of memory available on each node (in GB).
   :type total_memory_per_node: int
   :param spark_executors_cores: The number of cores to be allocated per Spark executor. Defaults to 5.
   :type spark_executors_cores: int, optional
   :param memory_fraction: The fraction of total memory per node to be allocated to each executor.
                           Defaults to 0.9 (i.e., 90%).
   :type memory_fraction: float, optional

   :returns:

             A dictionary containing the calculated Spark configuration with the following keys:
                 - `--executor-cores`: The number of cores to allocate per executor.
                 - `--executor-memory`: The amount of memory to allocate per executor (in GB).
                 - `--num-executors`: The total number of executor instances.
   :rtype: dict

   :raises ValueError: If any of the input parameters are invalid (e.g., non-positive values or insufficient cores per executor).

   .. rubric:: Example

   >>> config = spark_property_calculator(
           number_of_nodes=10,
           cores_per_node=16,
           total_memory_per_node=128,
           spark_executors_cores=4,
           memory_fraction=0.8
       )
   >>> print(config)
   {
       '--executor-cores': 4,
       '--executor-memory': '25G',
       '--num-executors': 39
   }

   In this example, the function calculates the optimal Spark configuration for a cluster with 10 nodes,
   each having 16 cores and 128 GB of memory. Each executor is allocated 4 cores, and 80% of the available memory
   is used per executor, resulting in 39 executors, each with 25 GB of memory and 4 cores.


.. py:function:: send_error_to_teams(error_message: str, webhook_url: str) -> None

   Function to send error messages to Microsoft Teams via webhook.

   :param error_message: The error message to be sent.
   :type error_message: str
   :param webhook_url: The webhook URL for Microsoft Teams.
   :type webhook_url: str

   .. rubric:: Example

   >>> send_error_to_teams('An error occurred', 'https://webhook_url.com')


.. py:function:: handle_exceptions(process: str, notebook_url: str, webhook_url: str) -> Any

   Context manager for handling exceptions and sending them to Microsoft Teams.

   :param process: The process that is being executed.
   :type process: str
   :param notebook_url: The URL of the notebook where the process is running.
   :type notebook_url: str
   :param webhook_url: The webhook URL for Microsoft Teams.
   :type webhook_url: str

   .. rubric:: Example

   >>> with handle_exceptions('Process Name', 'https://notebook_url.com', 'https://webhook_url.com'):
   >>>     # Your code here


