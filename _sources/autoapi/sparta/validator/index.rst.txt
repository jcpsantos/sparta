sparta.validator
================

.. py:module:: sparta.validator


Functions
---------

.. autoapisummary::

   sparta.validator.validator_typed_columns
   sparta.validator.validator_dataframe_columns
   sparta.validator.validate_column_types
   sparta.validator.compare_dataframes


Module Contents
---------------

.. py:function:: validator_typed_columns(typecase: str) -> str

   Validates the `typecase` argument to check if it is 'upper' or 'lower',
   which are accepted values for converting column case in a DataFrame.

   :param typecase: The case type to be validated. Accepted values are 'upper' or 'lower'.
   :type typecase: str

   :returns:

             'Ok' if the validation is successful. If the value is valid but improperly cased,
                  a warning is logged and 'Ok' is returned. If invalid, 'Error' is returned.
   :rtype: str

   .. rubric:: Example

   >>> validator_typed_columns('upper')
   'Ok'

   >>> validator_typed_columns('UPPER')
   # Logs a warning about case sensitivity
   'Ok'

   >>> validator_typed_columns('capitalize')
   'Error'


.. py:function:: validator_dataframe_columns(df: pyspark.sql.DataFrame, columns: List[Any], log: str = 'validator_dataframe_columns') -> None

   Validates whether the specified columns exist in the given DataFrame.
   It supports both string column names and PySpark's `F.col()` objects.

   :param df: The DataFrame in which the columns will be checked.
   :type df: DataFrame
   :param columns: A list of column names to be validated. Can be strings or `F.col()` objects.
   :type columns: List[Any]
   :param log: The logger identifier. Defaults to 'validator_dataframe_columns'.
   :type log: str, optional

   :raises ValueError: If one or more of the specified columns do not exist in the DataFrame.
   :raises TypeError: If an invalid column type (neither string nor `F.col()` object) is provided.

   .. rubric:: Example

   >>> from pyspark.sql import functions as F
   >>> df = spark.createDataFrame([(1, 'a'), (2, 'b')], ['id', 'value'])
   >>> validator_dataframe_columns(df, ['id', 'value'])
   # Logs a success message since both columns exist.

   >>> validator_dataframe_columns(df, [F.col('id'), 'value'])
   # Works with both string and F.col objects.

   >>> validator_dataframe_columns(df, ['non_existing_col'])
   # Raises a ValueError: "The columns {'non_existing_col'} do not exist in the dataframe."


.. py:function:: validate_column_types(df: pyspark.sql.DataFrame, expected_columns: Dict[Any, Any]) -> bool

   Validates that a DataFrame has the expected column names and data types.

   This function checks if the given DataFrame contains all the expected columns and whether
   their data types match the expected types. If any column is missing or has a different
   data type than expected, it logs an error message and returns `False`. If all columns
   match the expectations, it returns `True`.

   :param df: The Spark DataFrame to validate.
   :type df: DataFrame
   :param expected_columns: A dictionary where the keys are the expected column names
                            and the values are the expected data types (as strings).
   :type expected_columns: dict

   :returns:

             `True` if the DataFrame has all the expected columns with the correct data types,
                   `False` otherwise.
   :rtype: bool

   :raises None: This function does not raise exceptions, but logs errors using the logger
   :raises in case of validation failure.:

   .. rubric:: Example

   >>> expected_columns = {
       'name': 'string',
       'age': 'int',
       'salary': 'double'
   }
   >>> validate_column_types(df, expected_columns)


.. py:function:: compare_dataframes(df1: pyspark.sql.DataFrame, df2: pyspark.sql.DataFrame) -> bool

   Compares two PySpark DataFrames to check if they are different in terms of schema and content.

   Args:
   - df1: The first DataFrame.
   - df2: The second DataFrame.

   Return:
   - bool: False if the DataFrames are different, True otherwise.


