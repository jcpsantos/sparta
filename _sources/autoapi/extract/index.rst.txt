:py:mod:`extract`
=================

.. py:module:: extract


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   extract/index.rst


Package Contents
----------------


Functions
~~~~~~~~~

.. autoapisummary::

   extract.read_with_schema
   extract.read_yaml_df



.. py:function:: read_with_schema(path: str, schema: str, options: dict, format: str = 'parquet', spark: pyspark.sql.SparkSession = spark) -> pyspark.sql.DataFrame

   Function to read DataFrames with predefined schema.

   :param path: Path where the file is located.
   :type path: str
   :param schema: Pre-defined schema for reading.
   :type schema: str
   :param options: Configuration options for reading the DataFrame.
   :type options: dict
   :param format: Format of the file to be read. Defaults to 'parquet'.
   :type format: str, optional
   :param spark: Spark session. Defaults to spark.
   :type spark: SparkSession, optional

   :returns: DataFrame read with predefined schema.
   :rtype: DataFrame

   .. rubric:: Example

   >>> schema = 'epidemiological_week LONG, date DATE, order_for_place INT, state STRING, city STRING, city_ibge_code LONG, place_type STRING, last_available_confirmed INT'
       path = '/content/sample_data/covid19-e0534be4ad17411e81305aba2d9194d9.csv'
       df = read_with_schema(path, schema, {'header': 'true'}, 'csv')


.. py:function:: read_yaml_df(path: str, spark: pyspark.sql.SparkSession = spark) -> pyspark.sql.DataFrame

   Function to read a yaml file as a DataFrame.

   :param path: Path of the yaml file.
   :type path: str
   :param spark: Spark session. Defaults to spark.
   :type spark: SparkSession, optional

   :returns: Yaml file read and converted to a DataFrame.
   :rtype: DataFrame

   .. rubric:: Example

   >>> path = '/content/sample_data/schema_ingestao.yaml'
       df = read_yaml_df(path)


