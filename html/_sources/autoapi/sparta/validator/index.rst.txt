sparta.validator
================

.. py:module:: sparta.validator


Functions
---------

.. autoapisummary::

   sparta.validator.validator_typed_columns
   sparta.validator.validator_dataframe_columns
   sparta.validator.validate_column_types
   sparta.validator.compare_dataframes
   sparta.validator.monitoring_datetime


Module Contents
---------------

.. py:function:: validator_typed_columns(typecase: str) -> str

   Validates the `typecase` argument to check if it is 'upper' or 'lower',
   which are accepted values for converting column case in a DataFrame.

   :param typecase: The case type to be validated. Accepted values are 'upper' or 'lower'.
   :type typecase: str

   :returns:

             'Ok' if the validation is successful. If the value is valid but improperly cased,
                  a warning is logged and 'Ok' is returned. If invalid, 'Error' is returned.
   :rtype: str

   .. rubric:: Example

   >>> validator_typed_columns('upper')
   'Ok'

   >>> validator_typed_columns('UPPER')
   # Logs a warning about case sensitivity
   'Ok'

   >>> validator_typed_columns('capitalize')
   'Error'


.. py:function:: validator_dataframe_columns(df: pyspark.sql.DataFrame, columns: List[Any], log: str = 'validator_dataframe_columns') -> None

   Validates whether the specified columns exist in the given DataFrame.
   It supports both string column names and PySpark's `F.col()` objects.

   :param df: The DataFrame in which the columns will be checked.
   :type df: DataFrame
   :param columns: A list of column names to be validated. Can be strings or `F.col()` objects.
   :type columns: List[Any]
   :param log: The logger identifier. Defaults to 'validator_dataframe_columns'.
   :type log: str, optional

   :raises ValueError: If one or more of the specified columns do not exist in the DataFrame.
   :raises TypeError: If an invalid column type (neither string nor `F.col()` object) is provided.

   .. rubric:: Example

   >>> from pyspark.sql import functions as F
   >>> df = spark.createDataFrame([(1, 'a'), (2, 'b')], ['id', 'value'])
   >>> validator_dataframe_columns(df, ['id', 'value'])
   # Logs a success message since both columns exist.

   >>> validator_dataframe_columns(df, [F.col('id'), 'value'])
   # Works with both string and F.col objects.

   >>> validator_dataframe_columns(df, ['non_existing_col'])
   # Raises a ValueError: "The columns {'non_existing_col'} do not exist in the dataframe."


.. py:function:: validate_column_types(df: pyspark.sql.DataFrame, expected_columns: Dict[Any, Any]) -> bool

   Validates that a DataFrame has the expected column names and data types.

   This function checks if the given DataFrame contains all the expected columns and whether
   their data types match the expected types. If any column is missing or has a different
   data type than expected, it logs an error message and returns `False`. If all columns
   match the expectations, it returns `True`.

   :param df: The Spark DataFrame to validate.
   :type df: DataFrame
   :param expected_columns: A dictionary where the keys are the expected column names
                            and the values are the expected data types (as strings).
   :type expected_columns: dict

   :returns:

             `True` if the DataFrame has all the expected columns with the correct data types,
                   `False` otherwise.
   :rtype: bool

   :raises None: This function does not raise exceptions, but logs errors using the logger
   :raises in case of validation failure.:

   .. rubric:: Example

   >>> expected_columns = {
       'name': 'string',
       'age': 'int',
       'salary': 'double'
   }
   >>> validate_column_types(df, expected_columns)


.. py:function:: compare_dataframes(df1: pyspark.sql.DataFrame, df2: pyspark.sql.DataFrame) -> bool

   Compares two PySpark DataFrames to check if they are different in terms of schema and content.

   Args:
   - df1: The first DataFrame.
   - df2: The second DataFrame.

   Return:
   - bool: False if the DataFrames are different, True otherwise.


.. py:function:: monitoring_datetime(df, name, column_name, final_hour, webhook_url)

   Monitors the most recent timestamp in a specified column of a DataFrame
   and sends an alert via Microsoft Teams if the data is outdated.

   This function calculates the maximum value of a timestamp column in a given DataFrame,
   compares it with a threshold (current time minus a specified number of hours), and sends
   a notification to Microsoft Teams if the data has not been updated within the threshold.

   :param df: The input DataFrame containing the data to monitor.
   :type df: pyspark.sql.DataFrame
   :param name: The name of the process being monitored, used in the alert message.
   :type name: str
   :param column_name: The name of the timestamp column to monitor.
   :type column_name: str
   :param final_hour: The threshold in hours; data older than this will trigger an alert.
   :type final_hour: int
   :param webhook_url: The Microsoft Teams webhook URL for sending alerts.
   :type webhook_url: str

   :raises ValueError: If the specified column does not exist in the DataFrame.

   Side Effects:
       - Sends a notification to Microsoft Teams using the provided webhook URL.
       - Logs an informational message if an alert is triggered.

   .. rubric:: Example

   >>> from pyspark.sql import SparkSession
   >>> from pyspark.sql.functions import lit
   >>> spark = SparkSession.builder.getOrCreate()
   >>> df = spark.createDataFrame([(1, "2024-12-01 10:00:00")], ["id", "timestamp"])
   >>> df = df.withColumn("timestamp", lit("2024-12-01 10:00:00").cast("timestamp"))
   >>> monitoring_datetime(df, "MyProcess", "timestamp", 24, "https://example.webhook.url")


